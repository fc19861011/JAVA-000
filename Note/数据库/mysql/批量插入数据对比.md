## 一、存储过程(有序插入)

>测试环境：
>
>内存：4G
>
>CPU：Intel(R) Core(TM) i5-4590 CPU @ 3.30GHz
>
>mysql：5.7.29
>
>运行环境：docker swarm
>
>mysql的binlog日志没有开，autocommit关闭状态

### 1.1 单个事务多个Insert into语句的方式

#### 测试情况

| 每次事务提交的insert数量 | 执行时间 | 执行事务次数 |
| ------------------------ | -------- | ------------ |
| 1                        | 100.984s | 1000000      |
| 2                        | 68.951s  | 500000       |
| 5                        | 45.515s  | 200000       |
| 10                       | 38.573s  | 100000       |
| 50                       | 31.555s  | 20000        |
| 100                      | 30.137s  | 10000        |
| 500                      | 28.745s  | 2000         |
| 1000                     | 28.513s  | 1000         |
| 1500                     | 28.441s  | 667          |
| 1800                     | 28.348s  | 556          |
| 2000                     | 28.439s  | 500          |
| 5000                     | 28.296s  | 200          |
| 10000                    | 28.23s   | 100          |
| 100000                   | 28.01s   | 10           |
| 1000000                  | 28.07s   | 1            |

#### 存储过程代码

```sql
CREATE DEFINER=`mail`@`%` PROCEDURE `order_data_init3`( IN `batch_num` INT )
BEGIN
DECLARE
	i INT;
DECLARE
	total_num INT DEFAULT 1000000;

SET i = 0;
START TRANSACTION;
WHILE
	i < total_num DO
INSERT INTO order_info ( order_code, order_status, create_time, payment_time, delivery_time, expected_time, arrive_time, complete_time, merchant_id, merchant_name, customer_id, customer_name )
VALUES
	(
		concat( '234234023423', i ),
		'1',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		i,
		concat( 'merchant_name', i ),
		i,
		concat( 'customer_name', i ) 
	);
IF
	i % batch_num = 0 THEN
		COMMIT;
	START TRANSACTION;
END IF;
SET i = i + 1;
END WHILE;
COMMIT;
END
```

### 1.2 单个事务一个insert into xx values (),(),()...的方式

#### 测试情况

| insert into 后面的数据量 | 执行时间 | 执行事务次数 |
| ------------------------ | -------- | ------------ |
| 50                       | 27.63s   | 20000        |
| 100                      | 26.181s  | 10000        |
| 500                      | 27.456s  | 2000         |
| 1000                     | 31.954s  | 1000         |
| 1500                     | 36.244s  | 667          |
| 2000                     | 41.674s  | 500          |
| 5000                     | 67.447s  | 200          |
| 10000                    | 109.098s | 100          |

#### 存储过程代码

```sql
CREATE DEFINER = `mail` @`%` PROCEDURE `order_data_batch_init` ( IN `batch_num` INT ) BEGIN
DECLARE
	i INT;
DECLARE
	SQL_FOR_INSERT LONGBLOB;

SET i = 0;
WHILE
	i < 1000000 DO
IF
i % batch_num = 0 THEN
START TRANSACTION;

SET SQL_FOR_INSERT = concat( 'insert into order_info(order_code, order_status,', 'create_time, payment_time, delivery_time,', 'expected_time, arrive_time, complete_time,', 'merchant_id, merchant_name, customer_id,', 'customer_name) values ' );

SET SQL_FOR_INSERT = concat(
	SQL_FOR_INSERT,
	'(',
	concat( '''', '234234023423', i, '''' ),
	',',
	'''1''',
	',',
	REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
	',',
	REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
	',',
	REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
	',',
	REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
	',',
	REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
	',',
	REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
	',',
	concat( '''', i, '''' ),
	',',
	concat( '''', 'merchant_name', i, '''' ),
	',',
	concat( '''', i, '''' ),
	',',
	concat( '''', 'customer_name', i, '''' ),
	')' 
);
ELSE 
	SET SQL_FOR_INSERT = concat(
		SQL_FOR_INSERT,
		', (',
		concat( '''', '234234023423', i, '''' ),
		',',
		'''1''',
		',',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		',',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		',',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		',',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		',',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		',',
		REPLACE ( unix_timestamp( CURRENT_TIMESTAMP ( 3 ) ), '.', '' ),
		',',
		concat( '''', i, '''' ),
		',',
		concat( '''', 'merchant_name', i, '''' ),
		',',
		concat( '''', i, '''' ),
		',',
		concat( '''', 'customer_name', i, '''' ),
		')' 
	);

END IF;
IF
	( ( i + 1 ) % batch_num = 0 ) THEN
		
		SET @SQL = SQL_FOR_INSERT;
	PREPARE stmt 
	FROM
		@SQL;-- 预处理动态sql语句
	EXECUTE stmt;-- 执行sql语句
	DEALLOCATE PREPARE stmt;-- 释放prepare
	COMMIT;
	ELSEIF ( i = 999999 ) THEN
	SET @SQL = SQL_FOR_INSERT;
	PREPARE stmt 
	FROM
		@SQL;-- 预处理动态sql语句
	EXECUTE stmt;-- 执行sql语句
	DEALLOCATE PREPARE stmt;-- 释放prepare
	COMMIT;
END IF;
SET i = i + 1;
END WHILE;
END
```

### 1.3 总结

从结果来看：

批量提交多条insert into 语句 要比 insert into values 多个值 效率要高；

- 在理论环境下，在同一个事务中批量提交的越多，性能普遍较好（减少了创建事务的开销，但是会有触发log_buffer刷盘的开销），批量提交的时候需要注意控制事务的大小，事务太大，强制buffer flush的过程中，如果发生日志文件写满了，还没有写完，这样会导致日志不能切换，Mysql就会hang住。（这个时候可以根据文件修改时间来判断日志文件的旋转频率，旋转频率太频繁，说明日志文件较小）。
- insert into values (),()..如果values后面的值过多，会增加语法解析的时间，影响性能（对sql的解析引擎不友好）。

## 二、程序提交

### 2.1 单条提交

### 2.2 批量提交



总结：

事务的开销

redo日志的刷盘

实测：innodb_flush_log_at_trx_commit=2和innodb_flush_log_at_trx_commit=1在存储过程中批量插入时，性能相近

默认redo buffer大小8m，单挑Insert语句大概2.2k，这样8M大约可以存3000条左右的单条Insert 语句，

insert into values (),(),() 语法编译耗时较长， 执行计划 无法复用。

根据：redo日志刷盘的规则中的 当`log buffer`中已经使用的内存超过一半时  将会刷盘，那么设定批量提交1500条理论效果应该最好

网络io的损耗

## 三、参考资料：
https://www.cnblogs.com/zhiqian-ali/p/4976989.html 【insert批量插入优化方案】

https://blog.csdn.net/bohu83/article/details/82903976?utm_source=blogxgwz1 【MySQL · 源码分析 · 一条insert语句的执行过程】

https://yq.aliyun.com/articles/131279 【 **Mysql 批量insert 性能测试** 】

https://www.imooc.com/wenda/detail/582058【多个INSERT语句与具有多个VALUES的单个INSERT】

https://blog.csdn.net/cke63021/article/details/100222034【mysql innodb_log_file_size 和innodb_log_buffer_size参数】